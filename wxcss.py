import requests
from bs4 import BeautifulSoup
import cssutils
import os
from urllib.parse import urlparse

def extract_wechat_css(article_url, save_path="extracted_css"):
    """
    æå–å…¬ä¼—å·æ–‡ç« ä¸­çš„CSSæ ·å¼å¹¶ä¿å­˜åˆ°æ–‡ä»¶
    :param article_url: å…¬ä¼—å·æ–‡ç« å®Œæ•´é“¾æ¥ï¼ˆéœ€åŒ…å«http/httpsï¼‰
    :param save_path: CSSæ–‡ä»¶ä¿å­˜ç›®å½•ï¼Œé»˜è®¤å½“å‰ç›®å½•ä¸‹çš„extracted_cssæ–‡ä»¶å¤¹
    """
    # 1. éªŒè¯URLæœ‰æ•ˆæ€§
    parsed_url = urlparse(article_url)
    if not parsed_url.scheme or not parsed_url.netloc:
        print("âŒ é”™è¯¯ï¼šè¯·è¾“å…¥å®Œæ•´çš„å…¬ä¼—å·æ–‡ç« é“¾æ¥ï¼ˆå¦‚https://mp.weixin.qq.com/s/xxxï¼‰")
        return

    # 2. é…ç½®è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œé¿å…è¢«åçˆ¬æ‹¦æˆªï¼‰
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": "https://mp.weixin.qq.com/"
    }

    try:
        # 3. å‘é€è¯·æ±‚è·å–æ–‡ç« HTML
        print(f"ğŸ” æ­£åœ¨è¯·æ±‚æ–‡ç« ï¼š{article_url}")
        response = requests.get(article_url, headers=headers, timeout=10)
        response.raise_for_status()  # è‹¥çŠ¶æ€ç é200ï¼ŒæŠ›å‡ºHTTPError
        html_content = response.text

        # 4. è§£æHTMLæå–CSSï¼ˆåˆ†ä¸¤ç±»ï¼šå†…åµŒ<style>æ ‡ç­¾å’Œå¤–éƒ¨CSSé“¾æ¥ï¼‰
        soup = BeautifulSoup(html_content, "html.parser")
        all_css = ""

        # 4.1 æå–å†…åµŒ<style>æ ‡ç­¾ä¸­çš„CSS
        style_tags = soup.find_all("style")
        if style_tags:
            print(f"ğŸ“Œ æ‰¾åˆ° {len(style_tags)} ä¸ªå†…åµŒ<style>æ ‡ç­¾")
            for tag in style_tags:
                css_content = tag.get_text(strip=True)
                if css_content:
                    # ä½¿ç”¨cssutilsæ ¼å¼åŒ–CSSï¼ˆå»é™¤æ— æ•ˆå­—ç¬¦ï¼Œç»Ÿä¸€æ ¼å¼ï¼‰
                    parsed_css = cssutils.parseString(css_content)
                    all_css += cssutils.serialize(parsed_css, format="beautify") + "\n\n"

        # 4.2 æå–å¤–éƒ¨CSSé“¾æ¥ï¼ˆéƒ¨åˆ†å…¬ä¼—å·ä¼šå¼•ç”¨å¤–éƒ¨æ ·å¼æ–‡ä»¶ï¼‰
        link_tags = soup.find_all("link", rel="stylesheet", href=True)
        if link_tags:
            print(f"ğŸ“Œ æ‰¾åˆ° {len(link_tags)} ä¸ªå¤–éƒ¨CSSé“¾æ¥")
            for tag in link_tags:
                css_url = tag["href"]
                # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼ˆè¡¥å…¨ä¸ºå®Œæ•´URLï¼‰
                if not css_url.startswith(("http://", "https://")):
                    css_url = parsed_url.scheme + "://" + parsed_url.netloc + css_url

                try:
                    # è¯·æ±‚å¤–éƒ¨CSSæ–‡ä»¶
                    css_response = requests.get(css_url, headers=headers, timeout=5)
                    css_response.raise_for_status()
                    # æ ¼å¼åŒ–å¤–éƒ¨CSS
                    parsed_external_css = cssutils.parseString(css_response.text)
                    all_css += f"/* å¤–éƒ¨CSSï¼š{css_url} */\n"
                    all_css += cssutils.serialize(parsed_external_css, format="beautify") + "\n\n"
                except Exception as e:
                    print(f"âš ï¸ è·å–å¤–éƒ¨CSSå¤±è´¥ï¼ˆ{css_url}ï¼‰ï¼š{str(e)}")

        # 5. å¤„ç†æå–ç»“æœ
        if not all_css.strip():
            print("âŒ æœªæå–åˆ°ä»»ä½•CSSæ ·å¼ï¼Œå¯èƒ½åŸå› ï¼š")
            print("   1. æ–‡ç« é“¾æ¥æ— æ•ˆæˆ–å·²è¢«åˆ é™¤")
            print("   2. å…¬ä¼—å·å¼€å¯äº†ä¸¥æ ¼çš„åçˆ¬æœºåˆ¶")
            print("   3. æ–‡ç« æ— è‡ªå®šä¹‰CSSæ ·å¼")
            return

        # 6. ä¿å­˜CSSåˆ°æ–‡ä»¶
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆç”¨æ–‡ç« æ ‡é¢˜æˆ–é“¾æ¥åç¼€å‘½åï¼‰
        article_title = soup.title.get_text(strip=True) if soup.title else "wechat_article"
        # è¿‡æ»¤éæ³•æ–‡ä»¶åå­—ç¬¦
        valid_title = "".join([c for c in article_title if c not in '/\:*?"<>|'])[:50]  # é™åˆ¶é•¿åº¦
        css_filename = f"{valid_title}.css"
        css_filepath = os.path.join(save_path, css_filename)

        # å†™å…¥æ–‡ä»¶
        with open(css_filepath, "w", encoding="utf-8") as f:
            f.write(all_css)

        print(f"âœ… CSSæå–æˆåŠŸï¼æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼š\n{os.path.abspath(css_filepath)}")

    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPè¯·æ±‚é”™è¯¯ï¼š{e}ï¼ˆå¯èƒ½æ˜¯é“¾æ¥å¤±æ•ˆæˆ–æ— è®¿é—®æƒé™ï¼‰")
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼šè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯ï¼š{str(e)}")

# ------------------- æ‰§è¡Œå…¥å£ -------------------
if __name__ == "__main__":
    # 1. è¾“å…¥å…¬ä¼—å·æ–‡ç« é“¾æ¥ï¼ˆç¤ºä¾‹ï¼šæ›¿æ¢ä¸ºä½ éœ€è¦æå–çš„é“¾æ¥ï¼‰
    wechat_article_url = input("è¯·ç²˜è´´å…¬ä¼—å·æ–‡ç« å®Œæ•´é“¾æ¥ï¼š").strip()
    
    # 2. è°ƒç”¨å‡½æ•°æå–CSS
    extract_wechat_css(wechat_article_url)